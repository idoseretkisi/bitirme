{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "headed-massachusetts",
   "metadata": {},
   "source": [
    "# KODLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suburban-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from snowballstemmer import TurkishStemmer\n",
    "ts = TurkishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "banned-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.sabah.com.tr/yazarlar/arsiv')\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "figcaption = soup.section.find_all('figcaption')\n",
    "\n",
    "authorLinks = []\n",
    "authors = []\n",
    "\n",
    "for ahref in figcaption:\n",
    "    authors.append(ahref.get_text())\n",
    "    authorLinks.append('https://www.sabah.com.tr' + ahref.a.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "japanese-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_cp = {}\n",
    "for i in range(len(authors)):\n",
    "    try:\n",
    "        r = requests.get(authorLinks[i])\n",
    "    except (requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError) as err:\n",
    "        i -= 1\n",
    "        continue\n",
    "    else:\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "        div = soup.section.find('div', {'class' : 'row writerReadList custom04 yazar-infinite-load'})\n",
    "        if \"None\" in str(div):\n",
    "            i -= 1\n",
    "            continue\n",
    "        figcaption = div.find_all('figcaption')\n",
    "\n",
    "        cp = []\n",
    "\n",
    "        for ahref in figcaption:\n",
    "            cp.append('https://www.sabah.com.tr' + ahref.a.get('href'))\n",
    "        \n",
    "        a_cp[authors[i]] = cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "worldwide-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info</th>\n",
       "      <th>title</th>\n",
       "      <th>caption</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21 Şubat 2021, Pazar</td>\n",
       "      <td>MEHMET BARLAS</td>\n",
       "      <td>Dostlar da dost olmayanlar da Türkiye’nin neyi...</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/barlas/2021/...</td>\n",
       "      <td>\\n\\nCumhurbaşkanı Erdoğan'ın yönetimindeki Tür...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20 Şubat 2021, Cumartesi</td>\n",
       "      <td>MEHMET BARLAS</td>\n",
       "      <td>Muhalefetsiz demokrasi olmaz ama bu çeşit muha...</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/barlas/2021/...</td>\n",
       "      <td>\\n\\nSizler de merak etmiyor musunuz? CHP Genel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19 Şubat 2021, Cuma</td>\n",
       "      <td>MEHMET BARLAS</td>\n",
       "      <td>Her iki Türk’ten biri yurt dışına göç etmek mi...</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/barlas/2021/...</td>\n",
       "      <td>\\n\\nAlman gazetelerinden \"Der Tagesspiegel\", T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18 Şubat 2021, Perşembe</td>\n",
       "      <td>MEHMET BARLAS</td>\n",
       "      <td>Cumhurbaşkanı, demokrasi tarihinin en ağır suç...</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/barlas/2021/...</td>\n",
       "      <td>\\n\\nÇok partili demokrasiye geçtiğimiz günden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17 Şubat 2021, Çarşamba</td>\n",
       "      <td>MEHMET BARLAS</td>\n",
       "      <td>Artık gündemin ana maddesi, terörle sonuna kad...</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/barlas/2021/...</td>\n",
       "      <td>\\n\\nBelli ki Türkiye'nin ana gündemi artık ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>11 Şubat 2017, Cumartesi</td>\n",
       "      <td>ÖMER DURNA (AKDENİZ)</td>\n",
       "      <td>Önemli gün</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/bolgeler/ome...</td>\n",
       "      <td>\\n\\nBugün Antalya için çok önemli bir gün.\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>04 Şubat 2017, Cumartesi</td>\n",
       "      <td>ÖMER DURNA (AKDENİZ)</td>\n",
       "      <td>Acılar üstüne</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/bolgeler/ome...</td>\n",
       "      <td>\\n\\nAntalyamız bu hafta acılar yaşadı.\\r\\nAynı...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>28 Ocak 2017, Cumartesi</td>\n",
       "      <td>ÖMER DURNA (AKDENİZ)</td>\n",
       "      <td>Haydi maça</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/bolgeler/ome...</td>\n",
       "      <td>\\n\\nAntalya'da spor ve kardeşlik adına önemli ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>21 Ocak 2017, Cumartesi</td>\n",
       "      <td>ÖMER DURNA (AKDENİZ)</td>\n",
       "      <td>Laf-ı güzaf</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/bolgeler/ome...</td>\n",
       "      <td>\\n\\nDün bizim gazetenin manşet haberi Antalya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>14 Ocak 2017, Cumartesi</td>\n",
       "      <td>ÖMER DURNA (AKDENİZ)</td>\n",
       "      <td>İyi, kötü, çirkin</td>\n",
       "      <td>https://www.sabah.com.tr/yazarlar/bolgeler/ome...</td>\n",
       "      <td>\\n\\nAntalya güzellikleri ile dünyaya nam salan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2457 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          info                 title  \\\n",
       "0         21 Şubat 2021, Pazar         MEHMET BARLAS   \n",
       "1     20 Şubat 2021, Cumartesi         MEHMET BARLAS   \n",
       "2          19 Şubat 2021, Cuma         MEHMET BARLAS   \n",
       "3      18 Şubat 2021, Perşembe         MEHMET BARLAS   \n",
       "4      17 Şubat 2021, Çarşamba         MEHMET BARLAS   \n",
       "...                        ...                   ...   \n",
       "2452  11 Şubat 2017, Cumartesi  ÖMER DURNA (AKDENİZ)   \n",
       "2453  04 Şubat 2017, Cumartesi  ÖMER DURNA (AKDENİZ)   \n",
       "2454   28 Ocak 2017, Cumartesi  ÖMER DURNA (AKDENİZ)   \n",
       "2455   21 Ocak 2017, Cumartesi  ÖMER DURNA (AKDENİZ)   \n",
       "2456   14 Ocak 2017, Cumartesi  ÖMER DURNA (AKDENİZ)   \n",
       "\n",
       "                                                caption  \\\n",
       "0     Dostlar da dost olmayanlar da Türkiye’nin neyi...   \n",
       "1     Muhalefetsiz demokrasi olmaz ama bu çeşit muha...   \n",
       "2     Her iki Türk’ten biri yurt dışına göç etmek mi...   \n",
       "3     Cumhurbaşkanı, demokrasi tarihinin en ağır suç...   \n",
       "4     Artık gündemin ana maddesi, terörle sonuna kad...   \n",
       "...                                                 ...   \n",
       "2452                                         Önemli gün   \n",
       "2453                                      Acılar üstüne   \n",
       "2454                                         Haydi maça   \n",
       "2455                                        Laf-ı güzaf   \n",
       "2456                                  İyi, kötü, çirkin   \n",
       "\n",
       "                                                   link  \\\n",
       "0     https://www.sabah.com.tr/yazarlar/barlas/2021/...   \n",
       "1     https://www.sabah.com.tr/yazarlar/barlas/2021/...   \n",
       "2     https://www.sabah.com.tr/yazarlar/barlas/2021/...   \n",
       "3     https://www.sabah.com.tr/yazarlar/barlas/2021/...   \n",
       "4     https://www.sabah.com.tr/yazarlar/barlas/2021/...   \n",
       "...                                                 ...   \n",
       "2452  https://www.sabah.com.tr/yazarlar/bolgeler/ome...   \n",
       "2453  https://www.sabah.com.tr/yazarlar/bolgeler/ome...   \n",
       "2454  https://www.sabah.com.tr/yazarlar/bolgeler/ome...   \n",
       "2455  https://www.sabah.com.tr/yazarlar/bolgeler/ome...   \n",
       "2456  https://www.sabah.com.tr/yazarlar/bolgeler/ome...   \n",
       "\n",
       "                                                   text  \n",
       "0     \\n\\nCumhurbaşkanı Erdoğan'ın yönetimindeki Tür...  \n",
       "1     \\n\\nSizler de merak etmiyor musunuz? CHP Genel...  \n",
       "2     \\n\\nAlman gazetelerinden \"Der Tagesspiegel\", T...  \n",
       "3     \\n\\nÇok partili demokrasiye geçtiğimiz günden ...  \n",
       "4     \\n\\nBelli ki Türkiye'nin ana gündemi artık ter...  \n",
       "...                                                 ...  \n",
       "2452  \\n\\nBugün Antalya için çok önemli bir gün.\\r\\n...  \n",
       "2453  \\n\\nAntalyamız bu hafta acılar yaşadı.\\r\\nAynı...  \n",
       "2454  \\n\\nAntalya'da spor ve kardeşlik adına önemli ...  \n",
       "2455  \\n\\nDün bizim gazetenin manşet haberi Antalya ...  \n",
       "2456  \\n\\nAntalya güzellikleri ile dünyaya nam salan...  \n",
       "\n",
       "[2457 rows x 5 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for author in a_cp:\n",
    "    for i in range(len(a_cp[author])):\n",
    "        try:\n",
    "            r = requests.get(a_cp[author][i])\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError) as err:\n",
    "            i -= 1\n",
    "            continue\n",
    "        else:\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "            result = soup.find_all('div', attrs={'class':'col-sm-12'})\n",
    "            \n",
    "            if result[0].find('span', attrs={'class': 'postInfo'}) is None:\n",
    "                info = np.nan\n",
    "            else:\n",
    "                info =result[0].find('span', attrs={'class': 'postInfo'}).get_text()\n",
    "            \n",
    "            if result[0].find('span', attrs={'class': 'postInfo'}) is None:\n",
    "                title = np.nan\n",
    "            else:\n",
    "                title = result[0].find('strong', attrs={'class': 'postTitle'}).get_text()\n",
    "            \n",
    "            if result[0].find('span', attrs={'class': 'postInfo'}) is None:\n",
    "                caption = np.nan\n",
    "            else:\n",
    "                caption = result[0].find('h1', attrs={'class': 'postCaption'}).get_text()\n",
    "            \n",
    "            text = result[0].find('div', attrs={'class': 'newsBox'})\n",
    "            txt = text.get_text()\n",
    "            txt = txt.split(\"\\n\")\n",
    "\n",
    "            for j in range(0, len(txt) - 2):\n",
    "                if \"Bu köşe yazısını aşağıdaki linke tıklayarak sesli bir şekilde dinleyebilirsiniz\" in txt[j]:\n",
    "                    del txt[j]\n",
    "                elif  \" | \" in txt[j]:\n",
    "                    del txt[j]\n",
    "                \n",
    "            text = ''\n",
    "            for j in range(len(txt)):\n",
    "                text += \"\\n\" + txt[j] \n",
    "            \n",
    "            data.append((info, title, caption, a_cp[author][i], text))\n",
    "df = pd.DataFrame(data = data, columns = ['info', 'title', 'caption', 'link', 'text'])\n",
    "newData = df.to_csv('newData.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "announced-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "insured-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.lower()\n",
    "    text = \"\".join([i for i in text if (i.isalpha() or i is \" \" or i.isnumeric())])\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def wordTokenize(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('turkish')\n",
    "    stopwords.extend([\"da\", \"ki\", \"de\", \"bir\", \"kadar\", \"sonra\"])\n",
    "    text = nltk.word_tokenize(text)\n",
    "    for i in text:\n",
    "        if i in stopwords:\n",
    "            text.remove(i)\n",
    "    return text\n",
    "\n",
    "def sentTokenize(text):\n",
    "    text = nltk.sent_tokenize(text)\n",
    "    for i in range(len(text)):\n",
    "        text[i] = dataCleaning(text[i])\n",
    "    return text\n",
    "\n",
    "def wtLenDist(wt):\n",
    "    wt_len = [len(word) for word in wt]\n",
    "    wt_len_dist = dict(nltk.FreqDist(wt_len))\n",
    "    return wt_len_dist\n",
    "\n",
    "def stLenDist(st):\n",
    "    st_len = [len(nltk.word_tokenize(sent)) for sent in st]\n",
    "    st_len_dist = dict(nltk.FreqDist(st_len))\n",
    "    return st_len_dist\n",
    "\n",
    "def avgWtLen(wt):\n",
    "    return sum(len(word) for word in wt)/len(wt)\n",
    "\n",
    "def avgStLen(st):\n",
    "    return sum(len(nltk.word_tokenize(sent)) for sent in st)/len(st)\n",
    "\n",
    "def stemmer(wt):\n",
    "    return [ts.stemWord(word) for word in wt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pleased-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['text'].apply(lambda x : dataCleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ready-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_token'] = data['clean_text'].apply(lambda x : wordTokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "grand-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sent_token'] = data['text'].apply(lambda x : sentTokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "subject-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['wt_len_dist'] = data['word_token'].apply(lambda x : wtLenDist(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "negative-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['st_len_dist'] = data['sent_token'].apply(lambda x : stLenDist(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "electric-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['avg_wt_len'] = data['word_token'].apply(lambda x : avgWtLen(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "breathing-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['avg_st_len'] = data['sent_token'].apply(lambda x : avgStLen(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "christian-platform",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-9e2181250c53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stem_token'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ulkud\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4106\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4107\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4108\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-9e2181250c53>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stem_token'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-b09dcc71ff68>\u001b[0m in \u001b[0;36mstemmer\u001b[1;34m(wt)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstemWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-b09dcc71ff68>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstemWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ulkud\\Anaconda3\\lib\\site-packages\\snowballstemmer\\basestemmer.py\u001b[0m in \u001b[0;36mstemWord\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstemWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ulkud\\Anaconda3\\lib\\site-packages\\snowballstemmer\\turkish_stemmer.py\u001b[0m in \u001b[0;36m_stem\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1634\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[0mv_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1636\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__r_stem_noun_suffixes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit_backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ulkud\\Anaconda3\\lib\\site-packages\\snowballstemmer\\turkish_stemmer.py\u001b[0m in \u001b[0;36m__r_stem_noun_suffixes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1360\u001b[0m                     \u001b[0mv_22\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__r_mark_DA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1363\u001b[0m                             \u001b[1;32mraise\u001b[0m \u001b[0mlab45\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mlab44\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ulkud\\Anaconda3\\lib\\site-packages\\snowballstemmer\\turkish_stemmer.py\u001b[0m in \u001b[0;36m__r_mark_DA\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__r_check_vowel_harmony\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_among_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTurkishStemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma_6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ulkud\\Anaconda3\\lib\\site-packages\\snowballstemmer\\basestemmer.py\u001b[0m in \u001b[0;36mfind_among_b\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mcommon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcommon\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data['stem_token'] = data['word_token'].apply(lambda x : stemmer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-marketing",
   "metadata": {},
   "source": [
    "# DENEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "accredited-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "varied-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "info            string\n",
       "title           string\n",
       "caption         string\n",
       "link            string\n",
       "text            string\n",
       "clean_text      string\n",
       "word_token      object\n",
       "sent_token      object\n",
       "wt_len_dist     object\n",
       "st_len_dist     object\n",
       "stem_token      object\n",
       "avg_wt_len     Float64\n",
       "avg_st_len     Float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "thousand-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data.to_csv('Data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-technical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
